# log_analysis/auth.py
from django.conf import settings
import re
from datetime import datetime
from collections import defaultdict

from pyod.models.knn import KNN
import numpy as np

# def analyze_auth_logs(log_content):
#     pattern = r'(?P<timestamp>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}) IP=(?P<ip>\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}) User login attempt: username=(?P<username>\w+) status=(?P<status>\w+)(?: reason=(?P<reason>[\w\s]+))?'
    
#     ip_attempts = defaultdict(int)
#     user_attempts = defaultdict(int)
#     timestamps = []
    
#     lines = log_content.split('\n')
#     for line in lines:
#         if line.strip():
#             match = re.match(pattern, line)
#             if match:
#                 parsed_line = match.groupdict()
#                 ip_attempts[parsed_line['ip']] += 1
#                 user_attempts[parsed_line['username']] += 1
#                 timestamp = datetime.strptime(parsed_line['timestamp'], '%Y-%m-%d %H:%M:%S')
#                 timestamps.append(timestamp)
    
#     # Prepare data for PyOD - using IP attempt frequencies as the feature
#     ip_attempt_values = np.array(list(ip_attempts.values())).reshape(-1, 1)
    
#     # Initialize and fit the KNN model
#     if ip_attempt_values.shape[0] > 1:  # Ensure there's enough data to fit the model
#         knn_model = KNN()
#         knn_model.fit(ip_attempt_values)
        
#         # Detect outliers - IPs with abnormal attempt frequencies
#         ip_outliers = knn_model.labels_
        
#         # Map outliers back to IPs
#         outlier_ips = [ip for ip, outlier_flag in zip(ip_attempts.keys(), ip_outliers) if outlier_flag == 1]
#     else:
#         outlier_ips = []
    
#     analysis_results = {
#         'ip_attempts': dict(ip_attempts),
#         'user_attempts': dict(user_attempts),
#         'outlier_ips': outlier_ips,
#     }
    
#     return analysis_results

# test_dict = defaultdict(int)
# test_dict['non_existing_key'] += 1

# print(test_dict['non_existing_key'])  # Should print 1

def analyze_auth_logs(log_content):
    # Enhanced regular expression pattern to include IP addresses
    pattern = r'(?P<timestamp>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}) IP=(?P<ip>\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}) User login attempt: username=(?P<username>\w+) status=(?P<status>\w+)(?: reason=(?P<reason>[\w\s]+))?'
    
    # Initialize additional counters and structures
    ip_attempts = defaultdict(int)
    user_attempts = defaultdict(int)
    commonly_exploited_usernames = {'admin', 'administrator', 'root'}
    exploited_username_attempts = 0

    # Initialize counters for success and failure
    success_count = 0
    failure_count = 0
    failed_login_reasons = {}
    timestamps = []
    intervals_per_user = {}
    
    # Process each line of the log content
    lines = log_content.split('\n')
    for line in lines:
        if line.strip():  # Ensure the line is not empty
            match = re.match(pattern, line)
            if match:  # If the line matches the pattern
                parsed_line = match.groupdict()
                timestamp = datetime.strptime(parsed_line['timestamp'], '%Y-%m-%d %H:%M:%S')
                timestamps.append(timestamp)

                ip_attempts[parsed_line['ip']] += 1
                user_attempts[parsed_line['username']] += 1
                
                # Check for commonly exploited usernames
                if parsed_line['username'].lower() in commonly_exploited_usernames:
                    exploited_username_attempts += 1
                
                # Count successes and failures
                if parsed_line['status'] == 'success':
                    success_count += 1
                elif parsed_line['status'] == 'failure':
                    failure_count += 1
                    # Collect reasons for failures
                    reason = parsed_line.get('reason', 'Unknown')
                    failed_login_reasons[reason] = failed_login_reasons.get(reason, 0) + 1
                    
                    # Calculate the interval since the last attempt for each user
                    if len(timestamps) > 1:
                        interval = (timestamp - timestamps[-2]).total_seconds()
                        if parsed_line['username'] not in intervals_per_user:
                            intervals_per_user[parsed_line['username']] = []
                        intervals_per_user[parsed_line['username']].append(interval)

    # Compile the analysis results
    analysis_results = {
        'success_count': success_count,
        'failure_count': failure_count,
        'failed_login_reasons': failed_login_reasons,
        'average_intervals_per_user': {user: sum(intervals) / len(intervals) for user, intervals in intervals_per_user.items() if intervals},
        'exploited_username_attempts': exploited_username_attempts,
        'ip_attempts': dict(ip_attempts),
        'user_attempts': dict(user_attempts),
        'commonly_exploited_usernames': list(commonly_exploited_usernames),
    }

    return analysis_results
///////////////////////////////////////////////////

# def analyze_auth_logs(log_content):
#     # Initialize the dictionaries for analysis.
#     ip_attempts = defaultdict(int)
#     user_failures = defaultdict(int)
#     timestamps = defaultdict(list)
    
#     # Define the regular expression for log entry parsing.
#     pattern = r'(?P<timestamp>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}) IP=(?P<ip>\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}) User login attempt: username=(?P<username>\w+) status=(?P<status>\w+)(?: reason=(?P<reason>[\w\s]+))?'
    
#     # Process each log line to fill the dictionaries.
#     lines = log_content.split('\n')
#     for line in lines:
#         if line.strip():  # Skip empty lines.
#             match = re.match(pattern, line)
#             if match:
#                 parsed_line = match.groupdict()
#                 timestamp = datetime.strptime(parsed_line['timestamp'], '%Y-%m-%d %H:%M:%S')
#                 ip_attempts[parsed_line['ip']] += 1
#                 if parsed_line['status'] == 'failure':
#                     user_failures[parsed_line['username']] += 1
#                 timestamps[parsed_line['username']].append(timestamp)
    
#     # Analyze IP attempts.
#     ip_attempt_values = np.array(list(ip_attempts.values())).reshape(-1, 1)
#     outlier_ips = []
#     if ip_attempt_values.shape[0] > 1:
#         ip_model = KNN(n_neighbors=min(5, len(ip_attempt_values)-1))
#         ip_model.fit(ip_attempt_values)
#         ip_outliers = ip_model.labels_
#         outlier_ips = [ip for ip, outlier in zip(ip_attempts.keys(), ip_outliers) if outlier]
    
#     # Analyze user failures.
#     failure_values = np.array(list(user_failures.values())).reshape(-1, 1)
#     outlier_users_by_failures = []
#     if failure_values.shape[0] > 1:
#         failure_model = KNN(n_neighbors=min(5, len(failure_values)-1))
#         failure_model.fit(failure_values)
#         failure_outliers = failure_model.labels_
#         outlier_users_by_failures = [user for user, outlier in zip(user_failures.keys(), failure_outliers) if outlier]
    
#     # Compile the analysis results.
#     analysis_results = {
#         indicators = outlier_ips + outlier_users_by_failures
#         vulnerabilities_matched = query_nvd_for_vulnerabilities(indicators)
#         analysis_results['matched_vulnerabilities'] = vulnerabilities_matched
#         'ip_attempts': dict(ip_attempts),
#         'user_failures': dict(user_failures),
#         'outlier_ips': outlier_ips,
#         'outlier_users_by_failures': outlier_users_by_failures,
#         #'interval_outliers' can be added here following a similar process.
#     }
    
#     return analysis_results


# from collections import defaultdict, Counter

# def analyze_auth_logs(log_content):
#     pattern = r'(?P<timestamp>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}) IP=(?P<ip>\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}) User login attempt: username=(?P<username>\w+) status=(?P<status>\w+)(?: reason=(?P<reason>[\w\s]+))?'
    
#     ip_attempts = defaultdict(int)
#     user_attempts = defaultdict(int)
#     user_failures = defaultdict(int)
#     timestamps = defaultdict(list)
    
#     lines = log_content.split('\n')
#     for line in lines:
#         if line.strip():
#             match = re.match(pattern, line)
#             if match:
#                 parsed_line = match.groupdict()
#                 ip_attempts[parsed_line['ip']] += 1
#                 user_attempts[parsed_line['username']] += 1
#                 timestamp = datetime.strptime(parsed_line['timestamp'], '%Y-%m-%d %H:%M:%S')
#                 timestamps[parsed_line['username']].append(timestamp)
#                 if parsed_line['status'] == 'failure':
#                     user_failures[parsed_line['username']] += 1

#     # Analyze user failures
#     failure_values = np.array(list(user_failures.values())).reshape(-1, 1)
#     if failure_values.shape[0] > 1:  # Check if enough data to fit the model
#         failure_model = KNN(n_neighbors=min(5, failure_values.shape[0]-1))
#         failure_model.fit(failure_values)
#         failure_outliers = failure_model.labels_
#         outlier_users_by_failures = [user for user, outlier in zip(user_failures.keys(), failure_outliers) if outlier == 1]
#     else:
#         outlier_users_by_failures = []

#     # Analyze login intervals for each user
#     interval_outliers = []
#     for user, ts in timestamps.items():
#         if len(ts) > 2:  # Need at least 3 timestamps to calculate intervals
#             ts.sort()
#             intervals = np.diff(ts).astype('timedelta64[s]').astype(int).reshape(-1, 1)
#             interval_model = KNN(n_neighbors=min(5, intervals.shape[0]-1))
#             interval_model.fit(intervals)
#             if np.any(interval_model.labels_ == 1):  # If any interval is an outlier
#                 interval_outliers.append(user)

#     analysis_results = {
#         'ip_attempts': dict(ip_attempts),
#         'user_attempts': dict(user_attempts),
#         'outlier_ips': outlier_ips,
#         'outlier_users_by_failures': outlier_users_by_failures,
#         'interval_outliers': interval_outliers,
#     }
    
#     return analysis_results
///////////////////////////suite 
# Function to query the NVD for vulnerabilities based on indicators
# def query_nvd_for_vulnerabilities(indicators):
#     matched_vulnerabilities = []
#     base_url = "https://services.nvd.nist.gov/rest/json/cves/1.0"
    
#     for indicator in indicators:
#         try:
#                     # Prepare the query parameter for the NVD search
#             params = {"keyword": indicator, "resultsPerPage": "10"}  # Example limit to 10 results
#             response = requests.get(base_url, params=params)
            
#                     # Check if the request was successful
#             if response.status_code == 200:
#                 try:
#                     data = response.json()
#                     # Process each matched item to extract CVE ID
#                     for item in data.get('result', {}).get('CVE_Items', []):
#                         cve_id = item.get('cve', {}).get('CVE_data_meta', {}).get('ID', '')
#                         matched_vulnerabilities.append(cve_id)
#                 except ValueError:  # Includes simplejson.decoder.JSONDecodeError
#                     print(f"Error decoding JSON response for indicator {indicator}")
#             else:
#                 print(f"Non-200 status code received for indicator {indicator}: {response.status_code}")
#         except requests.exceptions.RequestException as e:
#             # Handle requests exceptions like timeouts, connection errors, etc.
#             print(f"Request error for indicator {indicator}: {str(e)}")
    
#     return matched_vulnerabilities

# def analyze_auth_logs(log_content):
#                 # Initialize dictionaries for analysis
#     ip_attempts = defaultdict(list)  # Collect timestamps of attempts per IP
#     user_failures = defaultdict(int)  # Count failures per user
    
#                 # Regular expression to parse log entries
#     log_entry_pattern = re.compile(
#         r'(?P<timestamp>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}) '
#         r'IP=(?P<ip>\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}) '
#         r'User login attempt: username=(?P<username>\w+) '
#         r'status=(?P<status>\w+)')
    
#     # Process each line in log content
#     for line in log_content.split('\n'):
#         match = log_entry_pattern.match(line)
#         if match:
#             data = match.groupdict()
#             # timestamp = datetime.strptime(data['timestamp'], '%Y-%m-%d %H:%M:%S')
#              isoformatted_timestamp = datetime.strptime(data['timestamp'], '%Y-%m-%d %H:%M:%S').isoformat()
#             ip_attempts[data['ip']].append(tisoformatted_timestamp)
#             if data['status'] == 'failure':
#                 user_failures[data['username']] += 1

#     # Convert data for PyOD analysis
#     # Example: Analyzing failure rates per user
#     failure_rates = np.array([v for v in user_failures.values()]).reshape(-1, 1)
    
#     # Initialize and fit KNN model for failure rates
#     if len(failure_rates) > 1:  # Ensure there's enough data
#         model = KNN()
#         model.fit(failure_rates)
#         outliers = model.labels_ == 1  # Assuming outliers are labeled as '1'

#         # Extract usernames corresponding to outliers
#         outlier_usernames = [user for user, is_outlier in zip(user_failures.keys(), outliers) if is_outlier]
#     else:
#         outlier_usernames = []

#     # Query the NVD for vulnerabilities based on outlier IPs and usernames
#     indicators = list(ip_attempts.keys()) + outlier_usernames
#     matched_vulnerabilities = query_nvd_for_vulnerabilities(indicators)

#     # Compile analysis results
#     analysis_results = {
#         'ip_attempts': dict(ip_attempts),
#         'user_failures': dict(user_failures),
#         'outlier_usernames': outlier_usernames,
#         # 'matched_vulnerabilities': matched_vulnerabilities,
#     }

#     return analysis_results
//////////////////////////////////////////////////////

# def query_nvd_for_vulnerabilities(indicators):
#     nvd_search_url = "https://services.nvd.nist.gov/rest/json/cves/2.0"
#     matched_vulnerabilities = []

#     for indicator in indicators:
#         query_params = {"keyword": indicator}
#         response = requests.get(nvd_search_url, params=query_params)
#         print(f"Status Code: {response.status_code}")
#         if response.status_code == 200:
#             try:
#                 vulnerabilities = response.json().get('result', {}).get('CVE_Items', [])
#                 matched_vulnerabilities.extend(vulnerabilities)
#             except ValueError as e:
#                 print(f"Error parsing JSON: {e}")
#                 print(f"Response Text: {response.text}")
#         else:
#             print(f"Failed to query NVD: {response.text}")

#     return matched_vulnerabilities

# def analyze_auth_logs(log_content):
#     # Initialize dictionaries for analysis
#     ip_attempts = defaultdict(int)
#     user_failures = defaultdict(int)
#     timestamps = defaultdict(list)
    
#     # Regular regex expression for log parsing
#     pattern = r'(?P<timestamp>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}) IP=(?P<ip>\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}) User login attempt: username=(?P<username>\w+) status=(?P<status>\w+)(?: reason=(?P<reason>[\w\s]+))?'
    
#     # traiter log lines
#     lines = log_content.split('\n')
#     for line in lines:
#         if line.strip():
#             match = re.match(pattern, line)
#             if match:
#                 parsed_line = match.groupdict()
#                 timestamp = datetime.strptime(parsed_line['timestamp'], '%Y-%m-%d %H:%M:%S')
#                 ip_attempts[parsed_line['ip']] += 1
#                 timestamps[parsed_line['username']].append(timestamp)
#                 if parsed_line['status'] == 'failure':
#                     user_failures[parsed_line['username']] += 1

#     # Analysis for outlier detection
#     ip_attempt_values = np.array(list(ip_attempts.values())).reshape(-1, 1)
#     outlier_ips = []
#     if ip_attempt_values.shape[0] > 1:
#         ip_model = KNN(n_neighbors=min(5, len(ip_attempt_values)-1))
#         ip_model.fit(ip_attempt_values)
#         ip_outliers = ip_model.labels_ == 1
#         outlier_ips = [ip for ip, outlier in zip(ip_attempts.keys(), ip_outliers) if outlier]

#     failure_values = np.array(list(user_failures.values())).reshape(-1, 1)
#     outlier_users_by_failures = []
#     if failure_values.shape[0] > 1:
#         failure_model = KNN(n_neighbors=min(5, len(failure_values)-1))
#         failure_model.fit(failure_values)
#         failure_outliers = failure_model.labels_ == 1
#         outlier_users_by_failures = [user for user, outlier in zip(user_failures.keys(), failure_outliers) if outlier]

#     # Combiner les indicators and query NVD
#     indicators = outlier_ips + outlier_users_by_failures
#     vulnerabilities_matched = query_nvd_for_vulnerabilities(indicators)

#     # Compiler analysis results
#     analysis_results = {
#         'ip_attempts': dict(ip_attempts),
#         'user_failures': dict(user_failures),
#         'outlier_ips': outlier_ips,
#         'outlier_users_by_failures': outlier_users_by_failures,
#         'matched_vulnerabilities': vulnerabilities_matched,
#     }

#     return analysis_results
    
# import re
# import requests
# import numpy as np
# from datetime import datetime
# from collections import defaultdict
# from pyod.models.knn import KNN

# def query_nvd_for_vulnerabilities(indicators):
#     base_url = "https://services.nvd.nist.gov/rest/json/cves/1.0"
#     matched_vulnerabilities = []
    
#     for indicator in indicators:
#         params = {"keyword": indicator, "resultsPerPage": "10"}
#         try:
#             response = requests.get(base_url, params=params)
#             response.raise_for_status()  # This will raise an exception for HTTP error codes
#             data = response.json()
#             for item in data.get('result', {}).get('CVE_Items', []):
#                 cve_id = item.get('cve', {}).get('CVE_data_meta', {}).get('ID', '')
#                 matched_vulnerabilities.append(cve_id)
#         except requests.exceptions.HTTPError as errh:
#             print("HTTP Error:", errh)
#         except requests.exceptions.ConnectionError as errc:
#             print("Error Connecting:", errc)
#         except requests.exceptions.Timeout as errt:
#             print("Timeout Error:", errt)
#         except requests.exceptions.RequestException as err:
#             print("Oops: Something Else", err)

#     return matched_vulnerabilities

# def analyze_auth_logs(log_content):
#     ip_attempts = defaultdict(int)
#     user_failures = defaultdict(int)
#     timestamps = defaultdict(list)

#     pattern = r'(?P<timestamp>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}) IP=(?P<ip>\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}) User login attempt: username=(?P<username>\w+) status=(?P<status>\w+)(?: reason=(?P<reason>[\w\s]+))?'

#     lines = log_content.split('\n')
#     for line in lines:
#         if line.strip():
#             match = re.match(pattern, line)
#             if match:
#                 parsed_line = match.groupdict()
#                 timestamp = datetime.strptime(parsed_line['timestamp'], '%Y-%m-%d %H:%M:%S')
#                 ip_attempts[parsed_line['ip']] += 1
#                 timestamps[parsed_line['username']].append(timestamp)
#                 if parsed_line['status'] == 'failure':
#                     user_failures[parsed_line['username']] += 1

#     # Outlier detection logic remains the same
#     ip_attempt_values = np.array(list(ip_attempts.values())).reshape(-1, 1)
#     outlier_ips = []
#     if ip_attempt_values.shape[0] > 1:
#         ip_model = KNN(n_neighbors=min(5, len(ip_attempt_values)-1))
#         ip_model.fit(ip_attempt_values)
#         ip_outliers = ip_model.labels_ == 1
#         outlier_ips = [ip for ip, outlier in zip(ip_attempts.keys(), ip_outliers) if outlier]

#     failure_values = np.array(list(user_failures.values())).reshape(-1, 1)
#     outlier_users_by_failures = []
#     if failure_values.shape[0] > 1:
#         failure_model = KNN(n_neighbors=min(5, len(failure_values)-1))
#         failure_model.fit(failure_values)
#         failure_outliers = failure_model.labels_ == 1
#         outlier_users_by_failures = [user for user, outlier in zip(user_failures.keys(), failure_outliers) if outlier]
#     # Skipping for brevity

#     # Query NVD for vulnerabilities based on indicators
#     indicators = outlier_ips + outlier_users_by_failures
#     # vulnerabilities_matched = query_nvd_for_vulnerabilities(indicators)

#     analysis_results = {
#         'ip_attempts': dict(ip_attempts),
#         'user_failures': dict(user_failures),
#         'outlier_ips': outlier_ips,
#         'outlier_users_by_failures': outlier_users_by_failures,
#         # 'matched_vulnerabilities': vulnerabilities_matched,
#     }

#     return analysis_results
#///////////////////////////////////////////////////////////auth
# from datetime import datetime
# from collections import defaultdict
# import re
# from sklearn.neighbors import NearestNeighbors as KNN
# import numpy as np

# def analyze_auth_logs(log_content):
#     ip_attempts = defaultdict(int)
#     user_failures = defaultdict(int)
#     timestamps = defaultdict(list)
#     ip_user_attempts = defaultdict(lambda: defaultdict(int))  # Tracks login attempts per user per IP for Credential Stuffing detection

#     pattern = r'(?P<timestamp>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}) IP=(?P<ip>\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}) User login attempt: username=(?P<username>\w+) status=(?P<status>\w+)(?: reason=(?P<reason>[\w\s]+))?'
    
#     detected_vulnerabilities = {}

#     lines = log_content.split('\n')
#     for line in lines:
#         if line.strip():
#             match = re.match(pattern, line)
#             if match:
#                 parsed_line = match.groupdict()
#                 timestamp = datetime.strptime(parsed_line['timestamp'], '%Y-%m-%d %H:%M:%S')
#                 ip_attempts[parsed_line['ip']] += 1
#                 timestamps[parsed_line['username']].append(timestamp)
#                 ip_user_attempts[parsed_line['ip']][parsed_line['username']] += 1
#                 if parsed_line['status'] == 'failure':
#                     user_failures[parsed_line['username']] += 1

#     # Analysis for each vulnerability type
#     # Brute Force Attack Detection
#     for user, count in user_failures.items():
#         if count > 6:  # Arbitrary threshold, adjust based on log volume and expected behavior
#             detected_vulnerabilities[user] = {"type": "Brute Force Attack", "description": "Numerous failed login attempts indicating a possible brute force attack."}

#     # Credential Stuffing Detection
#     for ip, users_attempts in ip_user_attempts.items():
#         if len(users_attempts) > 5:  # Indicates attempts across multiple accounts
#             detected_vulnerabilities[ip] = {"type": "Credential Stuffing", "description": "Failed login attempts across multiple usernames from the same IP address, suggesting credential stuffing."}

#     # Account Enumeration Detection
#     # Using simplistic logic here: if we see both successes and failures for user logins, it might indicate enumeration
#     account_enumeration = defaultdict(lambda: {'success': 0, 'failure': 0, 'first_success': None})
#     for line in lines:
#         if line.strip():
#             match = re.match(pattern, line)
#             if match:
#                 parsed_line = match.groupdict()
#                 status = parsed_line['status']
#                 user = parsed_line['username']
#                 timestamp = datetime.strptime(parsed_line['timestamp'], '%Y-%m-%d %H:%M:%S')
#                 if status == 'success':
#                     if account_enumeration[user]['success'] == 0:  # First successful login
#                         account_enumeration[user]['first_success'] = timestamp
#                     account_enumeration[user]['success'] += 1
#                 else:
#                     account_enumeration[user]['failure'] += 1
#     for user, info in account_enumeration.items():
#         if info['failure'] > 8 and info['first_success']:  # More than 5 failed attempts before the first success
#             detected_vulnerabilities[user] = {
#             "type": "Account Enumeration",
#             "description": "Multiple failed login attempts before a successful login, indicating potential account enumeration."
#         }
#     analysis_results = {
#         'ip_attempts': dict(ip_attempts),
#         'user_failures': dict(user_failures),
#         # 'outlier_ips': outlier_ips,
#         # 'outlier_users_by_failures': outlier_users_by_failures,
#         'detected_vulnerabilities': detected_vulnerabilities,
#     }

#     return analysis_results
#////////////////////////////////////////////////////// new func whith condition to show  or  vul 
# def analyze_application_logs(log_content):
#     # Definir les patt important de le fichier
#     pattern = r'(?P<timestamp>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}) IP=(?P<ip>\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}) UserID=(?P<userid>\w+) Action=(?P<action>[A-Za-z]+) (DocumentID=(?P<documentid>\d+))? Status=(?P<status>\w+)(?: Reason=(?P<reason>[\w\s]+))?'
    
#     action_counts = defaultdict(int)
#     status_counts = defaultdict(int)
#     user_actions = defaultdict(lambda: defaultdict(int))
#     document_access = defaultdict(lambda: defaultdict(int))
    
#     lines = log_content.split('\n')
#     for line in lines:
#         if line.strip():  # gerer not empty
#             match = re.match(pattern, line)
#              # If the line matches the pattern

#             if match: 
#                 parsed_line = match.groupdict()
#                 action = parsed_line['action']
#                 status = parsed_line['status']
#                 userid = parsed_line['userid']
#                 documentid = parsed_line.get('documentid')
                
#                 action_counts[action] += 1
#                 status_counts[status] += 1
#                 user_actions[userid][action] += 1
                
#                 if documentid:
#                     document_access[documentid][status] += 1
#     #compilatoin (ok)
#     analysis_results = {
#         'action_counts': dict(action_counts),
#         'status_counts': dict(status_counts),
#         'user_actions': {user: dict(actions) for user, actions in user_actions.items()},
#         'document_access': {doc: dict(access) for doc, access in document_access.items()},
#     }
    
#     return analysis_results




# old func ca fonctionnnnnnnnnne / 
# from django.http import HttpResponse
# def test_log_path(request):
#     app_log_path = settings.LOG_FILE_PATHS.get('application')
#     # Print the path to the console
#     print(app_log_path)
#     # Also return it as an HTTP response for testing
#     return HttpResponse(f"Application Log Path: {app_log_path}")

# def analyze_application_logs():
#     log_file_path = settings.LOG_FILE_PATHS.get('application')
#     try:
#         with open(log_file_path, 'r') as file:
#             log_content = file.read()
#             # Here, implement your logic for analyzing the application logs
#             # For simplicity, let's just return the first 100 characters
#             return log_content[:100]
#     except FileNotFoundError:
#         return "Application log file not found."
#     except Exception as e:
#         return str(e)
# def analyze_application_logs(log_content):
#     # Here, implement your logic for analyzing the application logs
#     # For simplicity, let's just return the first 100 characters of log_content
#     return log_content[:100]
//////////////////////////////////////////////////////////////
#///////ddos and sql

# def analyze_network_logs(log_content):
#     pattern = r'(?P<timestamp>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}) IP=(?P<src_ip>\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}) SrcPort=(?P<src_port>\d+) DestIP=(?P<dest_ip>\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}) DestPort=(?P<dest_port>\d+) Action=(?P<action>\w+) (Payload="(?P<payload>.+?)")? Status=(?P<status>\w+)(?: Reason=(?P<reason>.+))?'

#     ddos_attempts = defaultdict(int)
#     sql_injection_attempts = []
    
#     lines = log_content.split('\n')
#     for line in lines:
#         if line.strip():
#             match = re.match(pattern, line)
#             if match:
#                 parsed_line = match.groupdict()
#                 if parsed_line['dest_port'] == '80':
#                     ddos_attempts[parsed_line['dest_ip']] += 1
                
#                 if "SELECT" in parsed_line.get('payload', '') and "--" in parsed_line.get('payload', ''):
#                     sql_injection_attempts.append(parsed_line)
    
#     # Analyzer  DDoS en  counting requests per destination IP ca  marche pour le moment
#     potential_ddos = {dest_ip: count for dest_ip, count in ddos_attempts.items() if count > threshold}  
    
#     #compilation (ok)
#     analysis_results = {
#         'potential_ddos': potential_ddos,
#         'sql_injection_attempts': sql_injection_attempts,
#     }
    
#     return analysis_results
////////////////////////////////////////////// lat auth working 

from django.conf import settings
from datetime import datetime
from collections import defaultdict
import re

def analyze_auth_logs(log_content):
    ip_attempts = defaultdict(int)
    user_failures = defaultdict(int)
    timestamps = defaultdict(list)
    ip_user_attempts = defaultdict(lambda: defaultdict(int))  # Tracks login attempts per user per IP for Credential Stuffing detection
    account_enumeration = defaultdict(lambda: {'success': 0, 'failure': 0, 'first_success': None})  # Correctly defined here
    
    pattern = r'(?P<timestamp>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}) IP=(?P<ip>\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}) User login attempt: username=(?P<username>\w+) status=(?P<status>\w+)(?: reason=(?P<reason>[\w\s]+))?'

    detected_vulnerabilities = defaultdict(lambda: {"type": [], "description": []})

    lines = log_content.split('\n')
    for line in lines:
        if line.strip():
            match = re.match(pattern, line)
            if match:
                parsed_line = match.groupdict()
                timestamp = datetime.strptime(parsed_line['timestamp'], '%Y-%m-%d %H:%M:%S')
                ip_attempts[parsed_line['ip']] += 1
                timestamps[parsed_line['username']].append(timestamp)
                ip_user_attempts[parsed_line['ip']][parsed_line['username']] += 1
                if parsed_line['status'] == 'failure':
                    user_failures[parsed_line['username']] += 1
                    account_enumeration[parsed_line['username']]['failure'] += 1  # Update failure count
                else:
                    if not account_enumeration[parsed_line['username']]['first_success']:
                        account_enumeration[parsed_line['username']]['first_success'] = timestamp
                    account_enumeration[parsed_line['username']]['success'] += 1  # Update success count

    # Brute Force Attack Detection
    for user, count in user_failures.items():
        if count > 6:  # Adjust based on log volume and expected behavior
            detected_vulnerabilities[user]["type"].append("Brute Force Attack")
            detected_vulnerabilities[user]["description"].append("Numerous failed login attempts indicating a possible brute force attack.")

    # Credential Stuffing Detection
    # for ip, users_attempts in ip_user_attempts.items():
    #     if len(users_attempts) > 5:  # Indicates attempts across multiple accounts
    #         detected_vulnerabilities[ip]["type"].append("Credential Stuffing")
    #         detected_vulnerabilities[ip]["description"].append("Failed login attempts across multiple usernames from the same IP address, suggesting credential stuffing.")

    # Account Enumeration Detection
    # for user, info in account_enumeration.items():
    #     if info['failure'] > 8 and info['first_success']:  # a modifier threshold a fur et mesure
    #         detected_vulnerabilities[user]["type"].append("Account Enumeration")
    #         detected_vulnerabilities[user]["description"].append("Multiple failed login attempts before a successful login, indicating potential account enumeration. www.google.com")

    # Consolider le  results into a more informative structure
    analysis_results = {
        'ip_attempts': dict(ip_attempts),
        'user_failures': dict(user_failures),
        'detected_vulnerabilities': {user: {
                                        "vulnerabilities": detected_vulnerabilities[user]["type"],
                                        "descriptions": detected_vulnerabilities[user]["description"]
                                        } for user in detected_vulnerabilities}
    }

    return analysis_results
